\section*{Appendix A: Core Implementation}

The complete analysis is implemented in Python 3.12. This appendix presents the critical functions underlying the framework: correlated parameter sampling, the optimization solver, and the sensitivity analysis logic.

\subsection*{A.1 Correlated Monte Carlo Parameter Simulation}

This function generates 1,000 samples per site using Cholesky decomposition to impose realistic dependencies between conflicting parameters (e.g., urbanization vs. environmental baseline).

\begin{lstlisting}[language=Python, 
    caption={Correlated parameter sampling with truncated distributions},
    label={lst:monte_carlo},
    frame=single,
    basicstyle=\ttfamily\footnotesize,
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    captionpos=b]
def simulate_site_parameters(site_id: str, n_sim: int = 1000) -> pd.DataFrame:
    """
    Generate correlated parameters:
    - Capital cost vs Water: +0.50 (infrastructure access)
    - Population vs Environment: -0.60 (urban impact)
    """
    profile = SITE_PROFILES[site_id]
    
    # Pearson Correlation Matrix [Cost, Pop, Water, Env]
    corr_matrix = np.array([
        [1.00,  0.30,  0.50, -0.20],
        [0.30,  1.00, -0.10, -0.60],
        [0.50, -0.10,  1.00,  0.20],
        [-0.20, -0.60,  0.20,  1.00],
    ])
    
    # Cholesky decomposition for multivariate sampling
    L = np.linalg.cholesky(corr_matrix)
    uncorr = np.random.randn(n_sim, 4)
    corr_samples = uncorr @ L.T
    uniform_samples = stats.norm.cdf(corr_samples)
    
    # 1. Capital cost (Normal)
    cdist = CAPITAL_COST_DISTS[profile['capital_type']]
    capital = stats.norm.ppf(uniform_samples[:, 0], cdist['mean'], cdist['std'])
    capital = np.clip(capital, cdist['min'], cdist['max'])
    
    # 2. Population (Log-normal to handle demographic skewness)
    pdist = POPULATION_DISTS[profile['population_type']]
    mu_log = np.log(pdist['mean']**2 / np.sqrt(pdist['mean']**2 + pdist['std']**2))
    sig_log = np.sqrt(np.log(1 + (pdist['std']**2 / pdist['mean']**2)))
    population = stats.lognorm.ppf(uniform_samples[:, 1], s=sig_log, scale=np.exp(mu_log))
    population = np.clip(population, pdist['min'], pdist['max'])
    
    # 3. Water availability (Normal)
    wdist = WATER_DISTS[profile['water_type']]
    water = np.clip(stats.norm.ppf(uniform_samples[:, 2], wdist['mean'], wdist['std']),
                    wdist['min'], wdist['max'])
    
    # 4. Environmental quality (Normal)
    edist = ENV_QUALITY_DISTS[profile['env_type']]
    environment = np.clip(stats.norm.ppf(uniform_samples[:, 3], edist['mean'], edist['std']),
                         edist['min'], edist['max'])
    
    return pd.DataFrame({
        'site_id': site_id,
        'P_i': population.astype(int),
        'C_i': capital.round(1),
        'W_i': water.round(2),
        'E_i': environment.round(2),
    })
\end{lstlisting}

\subsection*{A.2 Goal Programming Optimization Solver}

Implements the lexicographic goal programming model as a Mixed Integer Linear Program (MILP) using the PuLP interface.

\begin{lstlisting}[language=Python,
    caption={Goal programming solver with deviation variables},
    label={lst:solver},
    frame=single,
    basicstyle=\ttfamily\footnotesize,
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    captionpos=b]
def solve_gp(sites: Dict, weights: Dict = None, system_params: Dict = None) -> Dict:
    """
    Minimizes weighted deviations from target goals:
    d1+: Population over-achievement (Safety)
    d2+: Budget overrun (Economics)
    d3-: Water shortfall (Feasibility)
    d4-: Quality shortfall (Compliance)
    """
    weights = weights or BASE_WEIGHTS
    params = system_params or SYSTEM_PARAMS
    
    model = LpProblem("Gabes_Relocation_GP", LpMinimize)
    
    # Binary selection variables
    x = {sid: LpVariable(f"x_{sid}", cat='Binary') for sid in sites}
    
    # Deviation variables (non-negative)
    d1_pos = LpVariable("d1_pos", lowBound=0) 
    d2_pos = LpVariable("d2_pos", lowBound=0) 
    d3_neg = LpVariable("d3_neg", lowBound=0) 
    d4_neg = LpVariable("d4_neg", lowBound=0) 
    
    # Objective: Lexicographic weighted minimization
    model += (weights['w1'] * d1_pos + weights['w2'] * d3_neg + 
              weights['w3'] * d2_pos + weights['w4'] * d4_neg)
    
    # Constraints
    model += lpSum([x[sid] for sid in sites]) == 1
    model += lpSum([sites[sid]['P_i'] * x[sid] for sid in sites]) == d1_pos
    model += lpSum([sites[sid]['TC_i'] * x[sid] for sid in sites]) - d2_pos == params['B']
    model += lpSum([sites[sid]['W_i'] * x[sid] for sid in sites]) + d3_neg >= params['W_req']
    model += lpSum([sites[sid]['E_i'] * x[sid] for sid in sites]) + d4_neg >= params['E_min']
    
    model.solve(PULP_CBC_CMD(msg=0))
    selected = [sid for sid in sites if value(x[sid]) == 1][0]
    
    return {'selected_site': selected, 'objective': value(model.objective)}
\end{lstlisting}

\section*{Appendix B: Software Environment and Reproducibility}

The analysis is implemented in \textbf{Python 3.12} using the following scientific stack:
\begin{itemize}
    \item \textbf{NumPy \& SciPy}: Stochastic sampling and statistical distributions.
    \item \textbf{Pandas}: Data manipulation and simulation result aggregation.
    \item \textbf{PuLP}: Linear programming interface using the COIN-OR CBC solver.
    \item \textbf{Matplotlib \& Seaborn}: Visualization of Pareto fronts and radar charts.
\end{itemize}



Full execution (24,000 iterations and 48 sensitivity scenarios) completes in approximately 12--15 seconds on a standard workstation. The random seed is fixed at 42 to ensure deterministic reproducibility of the Monte Carlo results.